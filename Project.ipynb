{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import datetime\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude):\n",
    "    lat1=pickup_longitude\n",
    "    lon1=pickup_latitude\n",
    "    lat2=dropoff_longitude\n",
    "    lon2=dropoff_latitude\n",
    "    \n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "\n",
    "    # convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # calculate differences between latitudes and longitudes\n",
    "    d_lat = lat2 - lat1\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    # calculate Haversine formula\n",
    "    a = math.sin(d_lat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(d_lon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "973199a4",
   "metadata": {},
   "source": [
    "Add Distance Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96447e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    dataframe['distance'] = dataframe.apply(lambda row: calculate_distance_with_coords(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the website page of the taxi_url\n",
    "def get_taxi_html():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the website page, find links for 'Yellow Taxi Trip Records'\n",
    "# From 2009 to 2015 monthly data\n",
    "def find_taxi_parquet_links():\n",
    "    get_taxi_html()\n",
    "    soup = bs4.BeautifulSoup(get_taxi_html(), 'html.parser')\n",
    "    l1 = soup.find_all(\"a\")\n",
    "    l2 = []\n",
    "    l3 = []\n",
    "    pattern = []\n",
    "    for i in range(len(l1)):\n",
    "        if l1[i].text == 'Yellow Taxi Trip Records':\n",
    "            l2.append(l1[i]['href'])\n",
    "    for j in range(len(l2)):\n",
    "        pattern.append(r\"2009-\\d{2}\")\n",
    "        pattern.append(r\"2010-\\d{2}\")\n",
    "        pattern.append(r\"2011-\\d{2}\")\n",
    "        pattern.append(r\"2012-\\d{2}\")\n",
    "        pattern.append(r\"2013-\\d{2}\")\n",
    "        pattern.append(r\"2014-\\d{2}\")\n",
    "        pattern.append(r\"2015-\\d{2}\")\n",
    "        for i in range(7):\n",
    "            if re.search(pattern[i], l2[j]):\n",
    "                l3.append(l2[j])\n",
    "                break\n",
    "        \n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the taxi zones data, process the data to show the longitude and latitude for a given location ID\n",
    "taxi_zones_df = gpd.read_file(\"taxi_zones/taxi_zones.shp\")\n",
    "taxi_zones_df = taxi_zones_df.to_crs(CRS)\n",
    "taxi_zones_df['longitude'] = taxi_zones_df.centroid.x  \n",
    "taxi_zones_df['latitude'] = taxi_zones_df.centroid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the url of the 'Yellow Taxi Trip Records', download the data, \n",
    "# Generate a sampling of Yellow Taxi data thatâ€™s roughly equal to the sample size of the Uber dataset\n",
    "# Select useful columns and rename them\n",
    "# Calculate duration of trip (trip end time - trip start time)\n",
    "# If the dataset only have location ID, select rows with valid location ID\n",
    "# change the location ID into longitude and latitude\n",
    "# Remove the data that is not within the specified limits\n",
    "# Change the measurement of duration to seconds, delete rows that have super long duration (outlier)\n",
    "# Delete rows with negative duration\n",
    "# Delete rows that have super long distance (outlier)\n",
    "# Delete rows with negative distance\n",
    "# Return the cleaned dataframe\n",
    "\n",
    "def process_dataframe(url, taxi_zones_df):\n",
    "    df = pd.read_parquet(url, engine='pyarrow')\n",
    "    df = df.sample(frac =0.0002)\n",
    "    if \"tpep_pickup_datetime\" not in df.columns:\n",
    "        if \"Trip_Pickup_DateTime\" in df.columns:\n",
    "            df = df[[\"Trip_Pickup_DateTime\", \"Trip_Dropoff_DateTime\", \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Tip_Amt\"]]\n",
    "            df.rename(columns = {'Start_Lon':'pickup_longitude', 'Start_Lat':'pickup_latitude',\n",
    "                                  'End_Lon':'dropoff_longitude', 'End_Lat':'dropoff_latitude'}, inplace = True)\n",
    "\n",
    "        elif \"pickup_datetime\" in df.columns:\n",
    "            df = df[[\"pickup_datetime\", \"dropoff_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\"]]\n",
    "            df.rename(columns = {'pickup_datetime':'Trip_Pickup_DateTime', 'dropoff_datetime':'Trip_Dropoff_DateTime',\n",
    "                                  'tip_amount':'Tip_Amt'}, inplace = True)  \n",
    "        \n",
    "        df['Trip_Pickup_DateTime'] = df['Trip_Pickup_DateTime'].apply(lambda x:datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "        df['Trip_Dropoff_DateTime'] = df['Trip_Dropoff_DateTime'].apply(lambda x:datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "        df['duration'] =  df['Trip_Dropoff_DateTime'] - df['Trip_Pickup_DateTime']\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        df = df[[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \"DOLocationID\", \"tip_amount\"]]\n",
    "        df = df[(df[\"PULocationID\"] < 263) & (df[\"PULocationID\"] > 0) \n",
    "               & (df[\"DOLocationID\"] < 263) & (df[\"DOLocationID\"] > 0)]\n",
    "        df = df.merge(taxi_zones_df[['LocationID','longitude','latitude']].set_index('LocationID'),\n",
    "                                    left_on='PULocationID', right_on='LocationID')\n",
    "        df = df.rename(columns={'longitude': 'pickup_longitude', 'latitude': 'pickup_latitude'})\n",
    "        df = df.merge(taxi_zones_df[['LocationID','longitude','latitude']].set_index('LocationID'),\n",
    "                                    left_on='DOLocationID', right_on='LocationID')\n",
    "        df = df.rename(columns={'longitude': 'dropoff_longitude', 'latitude': 'dropoff_latitude'})\n",
    "        df = df.drop(columns=[\"PULocationID\", \"DOLocationID\"])\n",
    "        df = df.rename(columns={'tpep_pickup_datetime': 'Trip_Pickup_DateTime', 'tpep_dropoff_datetime': 'Trip_Dropoff_DateTime', 'tip_amount': 'Tip_Amt'})\n",
    "        df['duration'] =  df['Trip_Dropoff_DateTime'] - df['Trip_Pickup_DateTime']\n",
    "        \n",
    "\n",
    "    westlimit=-74.242330; southlimit=40.560445; eastlimit=-73.717047; northlimit=40.908524\n",
    "    df = df[(df['pickup_longitude']<eastlimit) & (df['pickup_longitude']>westlimit)\n",
    "        & (df['pickup_latitude']<northlimit) & (df['pickup_latitude']>southlimit)\n",
    "        & (df['dropoff_longitude']<eastlimit) & (df['dropoff_longitude']>westlimit)\n",
    "        & (df['dropoff_latitude']<northlimit) & (df['dropoff_latitude']>southlimit)]\n",
    "    df['duration'] = df['duration'].apply(lambda x:x/np.timedelta64(1, 's'))\n",
    "    df = df[(df['duration'] <= 10000) & (df['duration'] > 0)]\n",
    "    df['distance'] = df.apply(lambda row: calculate_distance_with_coords(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
    "    df = df[(df['distance'] <= 100) & (df['distance'] > 0)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the full dataset for taxi data\n",
    "# Get dataframes from every month by function process_dataframe(url, taxi_zones_df) (defined above)\n",
    "# concat them together by time order to make the full dataset\n",
    "\n",
    "def get_full_data():\n",
    "    urls = find_taxi_parquet_links()\n",
    "    df = process_dataframe(urls[72], taxi_zones_df)\n",
    "    for i in range(73, 84):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])\n",
    "    for i in range(60, 72):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])       \n",
    "    for i in range(48, 60):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])\n",
    "    for i in range(36, 48):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])    \n",
    "    for i in range(24, 36):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])    \n",
    "    for i in range(12, 24):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])    \n",
    "    for i in range(0, 6):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_full_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    data_1 = pd.read_csv(csv_file)\n",
    "    data_2 = data_1.drop(columns=['Unnamed: 0','key','fare_amount','passenger_count'])\n",
    "    data_3 = data_2.dropna()\n",
    "    \n",
    "    # Define the range of acceptable latitude and longitude values\n",
    "    min_lat, max_lat = 40.560445, 40.908524\n",
    "    min_long, max_long = -74.242330, -73.717047\n",
    "    \n",
    "    # Remove trips that start and/or end outside of the latitude/longitude coordinate box\n",
    "    data_4 = data_3.drop(\n",
    "    index=data_3[\n",
    "    (data_3['pickup_latitude'] < min_lat) & \n",
    "    (data_3['pickup_latitude'] > max_lat) &\n",
    "    (data_3['pickup_longitude'] < min_long) &\n",
    "    (data_3['pickup_longitude'] > max_long) &\n",
    "    (data_3['dropoff_latitude'] < min_lat) &\n",
    "    (data_3['dropoff_latitude'] > max_lat) &\n",
    "    (data_3['dropoff_longitude'] < min_long) &\n",
    "    (data_3['dropoff_longitude'] > max_long)\n",
    "    ].index\n",
    ")\n",
    "    \n",
    "    #normalizing and using appropriate column types for the respective data\n",
    "    data_4['pickup_datetime'] = pd.to_datetime(data_3['pickup_datetime'])\n",
    "    data_4 = data_4.rename(columns = {'pickup_datetime': 'Trip_Pickup_DateTime'})\n",
    "    \n",
    "    return data_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_DATA)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    weather_data=pd.read_csv(csv_file)\n",
    "    weather_data['REPORT_TYPE'] = weather_data['REPORT_TYPE'].astype(str)\n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "    \n",
    "    # Filter out the rows where 'REPORT_TYPE' column contains the string 'SOD'\n",
    "    hourly_data = weather_data.loc[~weather_data['REPORT_TYPE'].str.contains('SOD')]\n",
    "\n",
    "    # Select only the required columns and rename them\n",
    "    hourly_data = hourly_data[['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "    hourly_data = hourly_data.rename(columns={'DATE': 'datetime', 'HourlyPrecipitation': 'hourly_precipitation', 'HourlyWindSpeed': 'hourly_wind_speed'})\n",
    "\n",
    "    # Convert 'HourlyPrecipitation' and 'HourlyWindSpeed' columns to numeric datatype\n",
    "    hourly_data[['hourly_precipitation', 'hourly_wind_speed']] = hourly_data[['hourly_precipitation', 'hourly_wind_speed']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fill NaN values in 'HourlyPrecipitation' and 'HourlyWindSpeed' columns with 0\n",
    "    hourly_data[['hourly_precipitation', 'hourly_wind_speed']] = hourly_data[['hourly_precipitation', 'hourly_wind_speed']].fillna(0)\n",
    "    \n",
    "    return hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):  \n",
    "    weather_data = pd.read_csv(csv_file)\n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE']).dt.date\n",
    "    \n",
    "     # Filter out the rows where 'REPORT_TYPE' column contains the string 'SOD'\n",
    "    hourly_data = weather_data.loc[~weather_data['REPORT_TYPE'].str.contains('SOD')]\n",
    "\n",
    "    # Select only the required columns\n",
    "    hourly_data = hourly_data[['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "    \n",
    "    # Convert the 'HourlyPrecipitation' and 'HourlyWindSpeed' columns to numeric format\n",
    "    hourly_data['HourlyPrecipitation'] = pd.to_numeric(hourly_data['HourlyPrecipitation'], errors='coerce').fillna(0)\n",
    "    hourly_data['HourlyWindSpeed'] = pd.to_numeric(hourly_data['HourlyWindSpeed'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Fill NaN values in 'HourlyPrecipitation' and 'HourlyWindSpeed' columns with 0\n",
    "    hourly_data['HourlyPrecipitation']=hourly_data['HourlyPrecipitation'].fillna(0)\n",
    "    hourly_data['HourlyWindSpeed']=hourly_data['HourlyWindSpeed'].fillna(0)\n",
    "    \n",
    "    # Aggregate the hourly data by date\n",
    "    hourly_data = hourly_data.groupby('DATE', as_index=False).agg({'HourlyWindSpeed': 'mean', \n",
    "                                                                   'HourlyPrecipitation': 'sum'})\n",
    "    \n",
    "    # Filter the daily data and select the 'DATE', 'Sunrise', and 'Sunset' columns\n",
    "    daily_data = weather_data[weather_data['REPORT_TYPE'] == 'SOD  ']\n",
    "    daily_data=daily_data[['DATE', 'Sunrise', 'Sunset']]\n",
    "    \n",
    "    # Convert the 'Sunrise' and 'Sunset' columns to numeric format\n",
    "    daily_data['Sunrise'] = pd.to_numeric(daily_data['Sunrise'], errors='coerce')\n",
    "    daily_data['Sunset'] = pd.to_numeric(daily_data['Sunset'], errors='coerce')\n",
    "    \n",
    "    # Merge the hourly and daily data on the 'DATE' column\n",
    "    daily_data = hourly_data.merge(daily_data, on='DATE', how='left')\n",
    "    \n",
    "    # Fill the data\n",
    "    daily_data.fillna(method='ffill',inplace=True)\n",
    "    daily_data.fillna(method='bfill',inplace=True)\n",
    "    \n",
    "    # Rename the columns.\n",
    "    daily_data = daily_data.rename(columns={'DATE': 'date', \n",
    "                                            'HourlyPrecipitation': 'daily_precipitation', \n",
    "                                            'HourlyWindSpeed': 'daily_wind_speed', \n",
    "                                            'Sunrise': 'sunrise', \n",
    "                                            'Sunset': 'sunset'})\n",
    "    \n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    datetime DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATE,\n",
    "    daily_wind_speed FLOAT,\n",
    "    daily_precipitation FLOAT\n",
    "    sunrise FLOAT\n",
    "    sunset FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    Trip_Pickup_DateTime DATETIME,\n",
    "    Trip_Dropoff_DateTime DATETIME,\n",
    "    Total_Amt FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    duration FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    Trip_Pickup_DateTime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25acf126",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for name, table in table_to_df_dict.items():\n",
    "        table.to_sql(name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT strftime('%H', Trip_Pickup_DateTime), count(*)\n",
    "FROM taxi_trips\n",
    "GROUP BY strftime('%H', Trip_Pickup_DateTime)\n",
    "ORDER BY count(*) DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"Popularity of taxi rides for each hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY4 = \"\"\"\n",
    "    WITH hired_trips AS\n",
    "    (\n",
    "    SELECT \n",
    "        Trip_Pickup_DateTime, \n",
    "        distance\n",
    "    FROM\n",
    "        taxi_trips\n",
    "    WHERE \n",
    "        strftime('%Y', Trip_Pickup_DateTime) = \"2009\"\n",
    "UNION\n",
    "    SELECT\n",
    "        Trip_Pickup_DateTime, \n",
    "        distance\n",
    "    FROM\n",
    "        uber_trips\n",
    "    WHERE \n",
    "        strftime('%Y', Trip_Pickup_DateTime) = \"2009\"\n",
    "    )\n",
    "    SELECT \n",
    "        date(Trip_Pickup_DateTime), count(*), AVG(distance)\n",
    "    FROM hired_trips\n",
    "    GROUP BY date(Trip_Pickup_DateTime)\n",
    "    ORDER BY count(*) DESC\n",
    "    LIMIT 10\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"Top 10 days with highest number of hired rides in 2009\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
