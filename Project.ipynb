{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import datetime\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude):\n",
    "    lat1=pickup_longitude\n",
    "    lon1=pickup_latitude\n",
    "    lat2=dropoff_longitude\n",
    "    lon2=dropoff_latitude\n",
    "    \n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "\n",
    "    # convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # calculate differences between latitudes and longitudes\n",
    "    d_lat = lat2 - lat1\n",
    "    d_lon = lon2 - lon1\n",
    "\n",
    "    # calculate Haversine formula\n",
    "    a = math.sin(d_lat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(d_lon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "973199a4",
   "metadata": {},
   "source": [
    "Add Distance Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96447e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    dataframe['distance'] = dataframe.apply(lambda row: calculate_distance_with_coords(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the website page of the taxi_url\n",
    "def get_taxi_html():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the website page, find links for 'Yellow Taxi Trip Records'\n",
    "# From 2009 to 2015 monthly data\n",
    "def find_taxi_parquet_links():\n",
    "    get_taxi_html()\n",
    "    soup = bs4.BeautifulSoup(get_taxi_html(), 'html.parser')\n",
    "    l1 = soup.find_all(\"a\")\n",
    "    l2 = []\n",
    "    l3 = []\n",
    "    pattern = []\n",
    "    for i in range(len(l1)):\n",
    "        if l1[i].text == 'Yellow Taxi Trip Records':\n",
    "            l2.append(l1[i]['href'])\n",
    "    for j in range(len(l2)):\n",
    "        pattern.append(r\"2009-\\d{2}\")\n",
    "        pattern.append(r\"2010-\\d{2}\")\n",
    "        pattern.append(r\"2011-\\d{2}\")\n",
    "        pattern.append(r\"2012-\\d{2}\")\n",
    "        pattern.append(r\"2013-\\d{2}\")\n",
    "        pattern.append(r\"2014-\\d{2}\")\n",
    "        pattern.append(r\"2015-\\d{2}\")\n",
    "        for i in range(7):\n",
    "            if re.search(pattern[i], l2[j]):\n",
    "                l3.append(l2[j])\n",
    "                break\n",
    "        \n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the taxi zones data, process the data to show the longitude and latitude for a given location ID\n",
    "taxi_zones_df = gpd.read_file(\"taxi_zones/taxi_zones.shp\")\n",
    "taxi_zones_df = taxi_zones_df.to_crs(CRS)\n",
    "taxi_zones_df['longitude'] = taxi_zones_df.centroid.x  \n",
    "taxi_zones_df['latitude'] = taxi_zones_df.centroid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the url of the 'Yellow Taxi Trip Records', download the data, \n",
    "# Generate a sampling of Yellow Taxi data thatâ€™s roughly equal to the sample size of the Uber dataset\n",
    "# Select useful columns and rename them\n",
    "# Calculate duration of trip (trip end time - trip start time)\n",
    "# If the dataset only have location ID, select rows with valid location ID\n",
    "# change the location ID into longitude and latitude\n",
    "# Remove the data that is not within the specified limits\n",
    "# Change the measurement of duration to seconds, delete rows that have super long duration (outlier)\n",
    "# Delete rows with negative duration\n",
    "# Delete rows that have super long distance (outlier)\n",
    "# Delete rows with negative distance\n",
    "# Return the cleaned dataframe\n",
    "\n",
    "def process_dataframe(url, taxi_zones_df):\n",
    "    df = pd.read_parquet(url, engine='pyarrow')\n",
    "    df = df.sample(frac =0.0002)\n",
    "    if \"tpep_pickup_datetime\" not in df.columns:\n",
    "        if \"Trip_Pickup_DateTime\" in df.columns:\n",
    "            df = df[[\"Trip_Pickup_DateTime\", \"Trip_Dropoff_DateTime\", \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\", \"Tip_Amt\"]]\n",
    "            df.rename(columns = {'Start_Lon':'pickup_longitude', 'Start_Lat':'pickup_latitude',\n",
    "                                  'End_Lon':'dropoff_longitude', 'End_Lat':'dropoff_latitude'}, inplace = True)\n",
    "\n",
    "        elif \"pickup_datetime\" in df.columns:\n",
    "            df = df[[\"pickup_datetime\", \"dropoff_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"tip_amount\"]]\n",
    "            df.rename(columns = {'pickup_datetime':'Trip_Pickup_DateTime', 'dropoff_datetime':'Trip_Dropoff_DateTime',\n",
    "                                  'tip_amount':'Tip_Amt'}, inplace = True)  \n",
    "        \n",
    "        df['Trip_Pickup_DateTime'] = df['Trip_Pickup_DateTime'].apply(lambda x:datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "        df['Trip_Dropoff_DateTime'] = df['Trip_Dropoff_DateTime'].apply(lambda x:datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "        df['duration'] =  df['Trip_Dropoff_DateTime'] - df['Trip_Pickup_DateTime']\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        df = df[[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \"DOLocationID\", \"tip_amount\"]]\n",
    "        df = df[(df[\"PULocationID\"] < 263) & (df[\"PULocationID\"] > 0) \n",
    "               & (df[\"DOLocationID\"] < 263) & (df[\"DOLocationID\"] > 0)]\n",
    "        df = df.merge(taxi_zones_df[['LocationID','longitude','latitude']].set_index('LocationID'),\n",
    "                                    left_on='PULocationID', right_on='LocationID')\n",
    "        df = df.rename(columns={'longitude': 'pickup_longitude', 'latitude': 'pickup_latitude'})\n",
    "        df = df.merge(taxi_zones_df[['LocationID','longitude','latitude']].set_index('LocationID'),\n",
    "                                    left_on='DOLocationID', right_on='LocationID')\n",
    "        df = df.rename(columns={'longitude': 'dropoff_longitude', 'latitude': 'dropoff_latitude'})\n",
    "        df = df.drop(columns=[\"PULocationID\", \"DOLocationID\"])\n",
    "        df = df.rename(columns={'tpep_pickup_datetime': 'Trip_Pickup_DateTime', 'tpep_dropoff_datetime': 'Trip_Dropoff_DateTime', 'tip_amount': 'Tip_Amt'})\n",
    "        df['duration'] =  df['Trip_Dropoff_DateTime'] - df['Trip_Pickup_DateTime']\n",
    "        \n",
    "\n",
    "    westlimit=-74.242330; southlimit=40.560445; eastlimit=-73.717047; northlimit=40.908524\n",
    "    df = df[(df['pickup_longitude']<eastlimit) & (df['pickup_longitude']>westlimit)\n",
    "        & (df['pickup_latitude']<northlimit) & (df['pickup_latitude']>southlimit)\n",
    "        & (df['dropoff_longitude']<eastlimit) & (df['dropoff_longitude']>westlimit)\n",
    "        & (df['dropoff_latitude']<northlimit) & (df['dropoff_latitude']>southlimit)]\n",
    "    df['duration'] = df['duration'].apply(lambda x:x/np.timedelta64(1, 's'))\n",
    "    df = df[(df['duration'] <= 10000) & (df['duration'] > 0)]\n",
    "    df['distance'] = df.apply(lambda row: calculate_distance_with_coords(row['pickup_latitude'], row['pickup_longitude'], row['dropoff_latitude'], row['dropoff_longitude']), axis=1)\n",
    "    df = df[(df['distance'] <= 100) & (df['distance'] > 0)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the full dataset for taxi data\n",
    "# Get dataframes from every month by function process_dataframe(url, taxi_zones_df) (defined above)\n",
    "# concat them together by time order to make the full dataset\n",
    "\n",
    "def get_full_data():\n",
    "    urls = find_taxi_parquet_links()\n",
    "    df = process_dataframe(urls[72], taxi_zones_df)\n",
    "    for i in range(73, 84):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])\n",
    "    for i in range(60, 72):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])       \n",
    "    for i in range(48, 60):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])\n",
    "    for i in range(36, 48):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])    \n",
    "    for i in range(24, 36):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])    \n",
    "    for i in range(12, 24):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])    \n",
    "    for i in range(0, 6):\n",
    "        df1 = process_dataframe(urls[i], taxi_zones_df)\n",
    "        df = pd.concat([df, df1])     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_full_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    data_1 = pd.read_csv(csv_file)\n",
    "    data_2 = data_1.drop(columns=['Unnamed: 0','key','fare_amount','passenger_count'])\n",
    "    data_3 = data_2.dropna()\n",
    "    \n",
    "    # Define the range of acceptable latitude and longitude values\n",
    "    min_lat, max_lat = 40.560445, 40.908524\n",
    "    min_long, max_long = -74.242330, -73.717047\n",
    "    \n",
    "    # Remove trips that start and/or end outside of the latitude/longitude coordinate box\n",
    "    data_4 = data_3.drop(\n",
    "    index=data_3[\n",
    "    (data_3['pickup_latitude'] < min_lat) & \n",
    "    (data_3['pickup_latitude'] > max_lat) &\n",
    "    (data_3['pickup_longitude'] < min_long) &\n",
    "    (data_3['pickup_longitude'] > max_long) &\n",
    "    (data_3['dropoff_latitude'] < min_lat) &\n",
    "    (data_3['dropoff_latitude'] > max_lat) &\n",
    "    (data_3['dropoff_longitude'] < min_long) &\n",
    "    (data_3['dropoff_longitude'] > max_long)\n",
    "    ].index\n",
    ")\n",
    "    \n",
    "    #normalizing and using appropriate column types for the respective data\n",
    "    data_4['pickup_datetime'] = pd.to_datetime(data_3['pickup_datetime'])\n",
    "    data_4 = data_4.rename(columns = {'pickup_datetime': 'Trip_Pickup_DateTime'})\n",
    "    \n",
    "    return data_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_DATA)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "UBER_DATA = \"uber_rides_sample.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    weather_data=pd.read_csv(csv_file)\n",
    "    weather_data['REPORT_TYPE'] = weather_data['REPORT_TYPE'].astype(str)\n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "    \n",
    "    # Filter out the rows where 'REPORT_TYPE' column contains the string 'SOD'\n",
    "    hourly_data = weather_data.loc[~weather_data['REPORT_TYPE'].str.contains('SOD')]\n",
    "\n",
    "    # Select only the required columns and rename them\n",
    "    hourly_data = hourly_data[['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "    hourly_data = hourly_data.rename(columns={'DATE': 'datetime', 'HourlyPrecipitation': 'hourly_precipitation', 'HourlyWindSpeed': 'hourly_wind_speed'})\n",
    "\n",
    "    # Convert 'HourlyPrecipitation' and 'HourlyWindSpeed' columns to numeric datatype\n",
    "    hourly_data[['hourly_precipitation', 'hourly_wind_speed']] = hourly_data[['hourly_precipitation', 'hourly_wind_speed']].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fill NaN values in 'HourlyPrecipitation' and 'HourlyWindSpeed' columns with 0\n",
    "    hourly_data[['hourly_precipitation', 'hourly_wind_speed']] = hourly_data[['hourly_precipitation', 'hourly_wind_speed']].fillna(0)\n",
    "    \n",
    "    return hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):  \n",
    "    weather_data = pd.read_csv(csv_file)\n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE']).dt.date\n",
    "    \n",
    "     # Filter out the rows where 'REPORT_TYPE' column contains the string 'SOD'\n",
    "    hourly_data = weather_data.loc[~weather_data['REPORT_TYPE'].str.contains('SOD')]\n",
    "\n",
    "    # Select only the required columns\n",
    "    hourly_data = hourly_data[['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']]\n",
    "    \n",
    "    # Convert the 'HourlyPrecipitation' and 'HourlyWindSpeed' columns to numeric format\n",
    "    hourly_data['HourlyPrecipitation'] = pd.to_numeric(hourly_data['HourlyPrecipitation'], errors='coerce').fillna(0)\n",
    "    hourly_data['HourlyWindSpeed'] = pd.to_numeric(hourly_data['HourlyWindSpeed'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Fill NaN values in 'HourlyPrecipitation' and 'HourlyWindSpeed' columns with 0\n",
    "    hourly_data['HourlyPrecipitation']=hourly_data['HourlyPrecipitation'].fillna(0)\n",
    "    hourly_data['HourlyWindSpeed']=hourly_data['HourlyWindSpeed'].fillna(0)\n",
    "    \n",
    "    # Aggregate the hourly data by date\n",
    "    hourly_data = hourly_data.groupby('DATE', as_index=False).agg({'HourlyWindSpeed': 'mean', \n",
    "                                                                   'HourlyPrecipitation': 'sum'})\n",
    "    \n",
    "    # Filter the daily data and select the 'DATE', 'Sunrise', and 'Sunset' columns\n",
    "    daily_data = weather_data[weather_data['REPORT_TYPE'] == 'SOD  ']\n",
    "    daily_data=daily_data[['DATE', 'Sunrise', 'Sunset']]\n",
    "    \n",
    "    # Convert the 'Sunrise' and 'Sunset' columns to numeric format\n",
    "    daily_data['Sunrise'] = pd.to_numeric(daily_data['Sunrise'], errors='coerce')\n",
    "    daily_data['Sunset'] = pd.to_numeric(daily_data['Sunset'], errors='coerce')\n",
    "    \n",
    "    # Merge the hourly and daily data on the 'DATE' column\n",
    "    daily_data = hourly_data.merge(daily_data, on='DATE', how='left')\n",
    "    \n",
    "    # Fill the data\n",
    "    daily_data.fillna(method='ffill',inplace=True)\n",
    "    daily_data.fillna(method='bfill',inplace=True)\n",
    "    \n",
    "    # Rename the columns.\n",
    "    daily_data = daily_data.rename(columns={'DATE': 'date', \n",
    "                                            'HourlyPrecipitation': 'daily_precipitation', \n",
    "                                            'HourlyWindSpeed': 'daily_wind_speed', \n",
    "                                            'Sunrise': 'sunrise', \n",
    "                                            'Sunset': 'sunset'})\n",
    "    \n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbdc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly2009=pd.DataFrame(clean_month_weather_data_hourly(\"weather data/2009_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d91aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly2010=pd.DataFrame(clean_month_weather_data_hourly(\"weather data/2010_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46775e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly2011=pd.DataFrame(clean_month_weather_data_hourly(\"weather data/2011_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly2012=pd.DataFrame(clean_month_weather_data_hourly(\"weather data/2012_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ac08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly2013=pd.DataFrame(clean_month_weather_data_hourly(\"weather data/2013_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly2014=pd.DataFrame(clean_month_weather_data_hourly(\"weather data/2014_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly2015=pd.DataFrame(clean_month_weather_data_hourly(\"weather data/2015_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf92e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data=pd.concat([hourly2009,hourly2010,hourly2011,hourly2012,hourly2013,hourly2014,hourly2015])\n",
    "hourly_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88dd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily2009=pd.DataFrame(clean_month_weather_data_daily(\"weather data/2009_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily2010=pd.DataFrame(clean_month_weather_data_daily(\"weather data/2010_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily2011=pd.DataFrame(clean_month_weather_data_daily(\"weather data/2011_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2232751",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily2012=pd.DataFrame(clean_month_weather_data_daily(\"weather data/2012_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37633796",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily2013=pd.DataFrame(clean_month_weather_data_daily(\"weather data/2013_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily2014=pd.DataFrame(clean_month_weather_data_daily(\"weather data/2014_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily2015=pd.DataFrame(clean_month_weather_data_daily(\"weather data/2015_weather.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_data=pd.concat([daily2009,daily2010,daily2011,daily2012,daily2013,daily2014,daily2015])\n",
    "daily_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    datetime DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATE,\n",
    "    daily_wind_speed FLOAT,\n",
    "    daily_precipitation FLOAT\n",
    "    sunrise FLOAT\n",
    "    sunset FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    Trip_Pickup_DateTime DATETIME,\n",
    "    Trip_Dropoff_DateTime DATETIME,\n",
    "    Total_Amt FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    duration FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    Trip_Pickup_DateTime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "SUN_DATA_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sun_data \n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATE,\n",
    "    sunrise INT32,\n",
    "    sunset INT32\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "    f.write(SUN_DATA_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25acf126",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    connection.execute(SUN_DATA_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for name, table in table_to_df_dict.items():\n",
    "        table.to_sql(name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "    \"sun_data\": sun_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT strftime('%H', Trip_Pickup_DateTime), count(*)\n",
    "FROM taxi_trips\n",
    "GROUP BY strftime('%H', Trip_Pickup_DateTime)\n",
    "ORDER BY count(*) DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"Popularity of taxi rides for each hour\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "458c67b2",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8130b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT strftime('%w', Trip_Pickup_DateTime) AS day, COUNT(*) AS number\n",
    "FROM uber_trips\n",
    "GROUP BY day\n",
    "ORDER BY number DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc469967",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"Popularity of uber rides for each day\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34dbafaa",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41cd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "    WITH hired_trips AS\n",
    "    (\n",
    "    SELECT \n",
    "        Trip_Pickup_DateTime, \n",
    "        distance\n",
    "    FROM\n",
    "        taxi_trips\n",
    "    WHERE \n",
    "        strftime('%m', Trip_Pickup_DateTime) = \"07\"\n",
    "    AND \n",
    "        strftime('%Y', Trip_Pickup_DateTime) = \"2013\"\n",
    "UNION\n",
    "    SELECT\n",
    "        Trip_Pickup_DateTime, \n",
    "        distance\n",
    "    FROM\n",
    "        uber_trips\n",
    "    WHERE \n",
    "        strftime('%m', Trip_Pickup_DateTime) = \"07\"\n",
    "    AND \n",
    "        strftime('%Y', Trip_Pickup_DateTime) = \"2013\"\n",
    "    )\n",
    "    SELECT distance FROM hired_trips ORDER BY distance\n",
    "    LIMIT 1 OFFSET ((select count(*) from hired_trips) * 95/100 -1)    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1995547",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057afad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"95 percentile distance by hired trips in July 2013\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4bec624",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"\n",
    "    WITH hired_trips AS\n",
    "    (\n",
    "    SELECT \n",
    "        Trip_Pickup_DateTime, \n",
    "        distance\n",
    "    FROM\n",
    "        taxi_trips\n",
    "    WHERE \n",
    "        strftime('%Y', Trip_Pickup_DateTime) = \"2009\"\n",
    "UNION\n",
    "    SELECT\n",
    "        Trip_Pickup_DateTime, \n",
    "        distance\n",
    "    FROM\n",
    "        uber_trips\n",
    "    WHERE \n",
    "        strftime('%Y', Trip_Pickup_DateTime) = \"2009\"\n",
    "    )\n",
    "    SELECT \n",
    "        date(Trip_Pickup_DateTime), count(*), AVG(distance)\n",
    "    FROM hired_trips\n",
    "    GROUP BY date(Trip_Pickup_DateTime)\n",
    "    ORDER BY count(*) DESC\n",
    "    LIMIT 10\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"Top 10 days with highest number of hired rides in 2009\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc3bb275",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5 = \"\"\"\n",
    "    WITH hired_trips AS\n",
    "    (\n",
    "    SELECT DATE(Trip_Pickup_DateTime) AS date \n",
    "    FROM taxi_trips\n",
    "    WHERE date BETWEEN '2014-01-01' AND '2015-01-01'\n",
    "UNION\n",
    "    SELECT DATE(Trip_Pickup_DateTime) AS date\n",
    "    FROM uber_trips\n",
    "    WHERE date BETWEEN '2014-01-01' AND '2015-01-01'\n",
    ")\n",
    "    SELECT \n",
    "        hired_trips.date as date, printf(\"%.4f\",daily_wind_speed), COUNT(*) AS number\n",
    "    FROM hired_trips\n",
    "    JOIN \n",
    "        daily_weather ON hired_trips.date = DATE(daily_weather.date)\n",
    "    GROUP BY hired_trips.date\n",
    "    ORDER BY daily_wind_speed DESC\n",
    "    LIMIT 10\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff12e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"Top 10 Windiest Date with number of hired trips in 2014\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b5d2c8c",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50fecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "    WITH hired_trips AS\n",
    "    (\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d %H',Trip_Pickup_DateTime) AS trip_hour \n",
    "    FROM \n",
    "        taxi_trips\n",
    "    WHERE \n",
    "        Trip_Pickup_DateTime BETWEEN '2012-10-22' AND '2012-10-31'\n",
    "UNION ALL\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d %H',Trip_Pickup_DateTime) AS trip_hour \n",
    "    FROM \n",
    "        uber_trips\n",
    "    WHERE \n",
    "        Trip_Pickup_DateTime BETWEEN '2012-10-22' AND '2012-10-31'\n",
    "        ),\n",
    "    hurricane_weather AS\n",
    "    (\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d %H',datetime) AS weather_hour, hourly_precipitation, hourly_wind_speed \n",
    "    FROM hourly_weather\n",
    "    WHERE \n",
    "        datetime BETWEEN '2012-10-22' AND '2012-10-31'\n",
    "    )\n",
    "    SELECT \n",
    "        weather_hour, COALESCE(COUNT(trip_hour),0) AS number, hourly_precipitation, hourly_wind_speed\n",
    "    FROM hurricane_weather\n",
    "    LEFT JOIN hired_trips\n",
    "    ON trip_hour = weather_hour\n",
    "    GROUP BY weather_hour\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"Harrican Weather Data with number of hired trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    df1 = pd.read_sql_query(QUERY_1, engine) \n",
    "    df1 = df1.sort_values(by=[\"strftime('%H', Trip_Pickup_DateTime)\"])\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_popularity_hourly(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    plt.bar(df1[\"strftime('%H', Trip_Pickup_DateTime)\"], df1[\"count(*)\"])\n",
    "    \n",
    "    axes.set_title(\"Popularity of taxi rides for each hour\")\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(\"Number of Taxi Rides\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_data_for_visual_1()\n",
    "plot_popularity_hourly(df1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85e53b0a",
   "metadata": {},
   "source": [
    "### Visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_3():\n",
    "    QUERY_LGA = \"\"\"\n",
    "    WITH hired_trips AS\n",
    "    (\n",
    "    SELECT \n",
    "        strftime('%w', Trip_Pickup_DateTime) + 1 AS day_of_week, \n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude\n",
    "    FROM\n",
    "        taxi_trips\n",
    "    WHERE dropoff_longitude < -73.854838 AND dropoff_longitude > -73.891745\n",
    "        AND dropoff_latitude < 40.778865 AND dropoff_latitude > 40.763589\n",
    "\n",
    "UNION\n",
    "    SELECT\n",
    "        strftime('%w', Trip_Pickup_DateTime) + 1 AS day_of_week, \n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude\n",
    "    FROM\n",
    "        uber_trips\n",
    "    WHERE dropoff_longitude < -73.854838 AND dropoff_longitude > -73.891745\n",
    "        AND dropoff_latitude < 40.778865 AND dropoff_latitude > 40.763589\n",
    "    )\n",
    "    SELECT \n",
    "         day_of_week, count(*) AS LGA\n",
    "    FROM hired_trips\n",
    "    GROUP BY day_of_week\n",
    "    ORDER BY day_of_week\n",
    "\n",
    "\"\"\"\n",
    "    QUERY_JFK = \"\"\"\n",
    "    WITH hired_trips AS\n",
    "    (\n",
    "    SELECT \n",
    "        strftime('%w', Trip_Pickup_DateTime) + 1 AS day_of_week, \n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude\n",
    "    FROM\n",
    "        taxi_trips\n",
    "    WHERE dropoff_longitude < -73.766264 AND dropoff_longitude > -73.795642\n",
    "        AND dropoff_latitude < 40.651376 AND dropoff_latitude > 40.639263\n",
    "\n",
    "UNION\n",
    "    SELECT\n",
    "        strftime('%w', Trip_Pickup_DateTime) + 1 AS day_of_week, \n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude\n",
    "    FROM\n",
    "        uber_trips\n",
    "    WHERE dropoff_longitude < -73.766264 AND dropoff_longitude > -73.795642\n",
    "        AND dropoff_latitude < 40.651376 AND dropoff_latitude > 40.639263\n",
    "    )\n",
    "    SELECT \n",
    "         day_of_week, count(*) AS JFK\n",
    "    FROM hired_trips\n",
    "    GROUP BY day_of_week\n",
    "    ORDER BY day_of_week\n",
    "\n",
    "\"\"\"\n",
    "    QUERY_EWR = \"\"\"\n",
    "    WITH hired_trips AS\n",
    "    (\n",
    "    SELECT \n",
    "        strftime('%w', Trip_Pickup_DateTime) + 1 AS day_of_week, \n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude\n",
    "    FROM\n",
    "        taxi_trips\n",
    "    WHERE dropoff_longitude < -74.165205 AND dropoff_longitude > -74.194028\n",
    "        AND dropoff_latitude < 40.699680 AND dropoff_latitude > 40.686794\n",
    "\n",
    "UNION\n",
    "    SELECT\n",
    "        strftime('%w', Trip_Pickup_DateTime) + 1 AS day_of_week, \n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude\n",
    "    FROM\n",
    "        uber_trips\n",
    "    WHERE dropoff_longitude < -74.165205 AND dropoff_longitude > -74.194028\n",
    "        AND dropoff_latitude < 40.699680 AND dropoff_latitude > 40.686794\n",
    "    )\n",
    "    SELECT \n",
    "         day_of_week, count(*) AS EWR\n",
    "    FROM hired_trips\n",
    "    GROUP BY day_of_week\n",
    "    ORDER BY day_of_week\n",
    "\n",
    "\"\"\"\n",
    "    df1 = pd.read_sql_query(QUERY_LGA, engine) \n",
    "    df2 = pd.read_sql_query(QUERY_JFK, engine) \n",
    "    df3 = pd.read_sql_query(QUERY_EWR, engine) \n",
    "    df4 = pd.merge(df1, df2, how = \"left\", on = [\"day_of_week\", \"day_of_week\"])\n",
    "    df5 = pd.merge(df4, df3, how = \"left\", on = [\"day_of_week\", \"day_of_week\"])\n",
    "    return df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_popularity_hourly(dataframe):\n",
    "    dataframe.plot(x=\"day_of_week\", y=[\"LGA\", \"JFK\", \"EWR\"], kind=\"bar\", \n",
    "                   title = \"Dropoffs for Each Airport in Each Week Day\",\n",
    "                  xlabel = \"Week Day\", ylabel = \"Number of Dropoffs\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = get_data_for_visual_3()\n",
    "plot_popularity_hourly(df3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8c61ef",
   "metadata": {},
   "source": [
    "### Visualization 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee97683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_5():\n",
    "    QUERY = \"\"\"\n",
    "    SELECT Tip_Amt, distance FROM taxi_trips\n",
    "    \"\"\"\n",
    "    df1 = pd.read_sql_query(QUERY, engine) \n",
    "    df1 = df1.sample(2000)\n",
    "    # only show 2000 data so we can see the trend more clearly\n",
    "    df1 = df1[df1[\"Tip_Amt\"] < 50]\n",
    "    # delete outliers with tip amount greater than 50\n",
    "    df1 = df1[df1[\"distance\"] < 50]\n",
    "    # delete outliers with distance greater than 50\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tip_Amt_VS_Distance(dataframe):\n",
    "    dataframe.plot(x=\"distance\", y=\"Tip_Amt\", kind=\"scatter\", \n",
    "                   title = \"Tip Amount Versus Distance for Yellow Taxi Rides\",\n",
    "                  xlabel = \"Distance\", ylabel = \"Tip Amount\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = get_data_for_visual_5()\n",
    "Tip_Amt_VS_Distance(df5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d1af150",
   "metadata": {},
   "source": [
    "Visualization 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9cfbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_tip =\"\"\"\n",
    "    SELECT strftime('%Y-%m-%d %H', Trip_Pickup_DateTime) AS DATE, Tip_Amt\n",
    "    FROM taxi_trips\n",
    "          \"\"\" \n",
    "QUERY_precipitation =\"\"\"\n",
    "    SELECT strftime('%Y-%m-%d %H', datetime) as DATE, hourly_precipitation\n",
    "    FROM hourly_weather\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90df1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tip_precipitation():\n",
    "    # Load tip and precipitation data into Pandas DataFrames from a database using an engine object\n",
    "    df_tip = pd.read_sql_query(QUERY_tip, engine)\n",
    "    df_precipitation =pd.read_sql_query(QUERY_precipitation, engine)\n",
    "    \n",
    "    # Merge tip and precipitation data on the 'DATE' column and sample 2000 rows\n",
    "    df = df_tip.merge(df_precipitation,on = 'DATE').sample(2000)\n",
    "    \n",
    "    # Filter rows where hourly precipitation and tip amount are greater than 0\n",
    "    df = df[(df['hourly_precipitation'] > 0) & (df['Tip_Amt'] > 0)]\n",
    "    \n",
    "    # Plot a scatter plot of tip amount vs. hourly precipitation\n",
    "    df.plot(x=\"Tip_Amt\", y=\"hourly_precipitation\", kind=\"scatter\", \n",
    "            title=\"TipAmt VS. Precipitation for Yellow Taxi rides\", \n",
    "            xlabel=\"Tip Amount\", ylabel=\"Hourly Precipitation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce35e068",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z1/_1bxs8qj3xs401mq4d3s_5240000gp/T/ipykernel_14799/1380364975.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtip_precipitation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/z1/_1bxs8qj3xs401mq4d3s_5240000gp/T/ipykernel_14799/2595070570.py\u001b[0m in \u001b[0;36mtip_precipitation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtip_precipitation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Load tip and precipitation data into Pandas DataFrames from a database using an engine object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf_tip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQUERY_tip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf_precipitation\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQUERY_precipitation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "tip_precipitation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3d8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
